{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                                     # Reading the Data #\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e05263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd # 导入pandas库，用pd作别名\n",
    "HOUSING_PATH = os.path.join(\"datasets\",\"housing\") # 指定下载数据到的目录 datasets/housing/\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path,\"housing.csv\")\n",
    "    return pd.read_csv(csv_path) # 返回一个包含所有数据的 pandas 数据框架对象\n",
    "housing = load_housing_data() # 返回一个包含所有数据的 pandas 数据框架对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98e199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                                     # Create a Test set #\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7eadbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分层抽样 根据收入中位数来进行分层抽样\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 将 median_income 的数据按照bins来划分，并给每个区间中的数据打上label\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],bins=[0.,1.5,3,4.5,6.,np.inf],labels=[1,2,3,4,5])\n",
    "# 现在层次已经分出来了，我们要做的就是在每一层选择一些样本出来\n",
    "# sklearn 提供了 StratifiedShuffleSplit 函数来进行分层抽样\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# n_splits 表示只分成一对 test/train 集，返回的还是一个 StratifiedShuffleSplit 对象\n",
    "split_obj = StratifiedShuffleSplit(n_splits=1,test_size=0.2, random_state=42)\n",
    "# split_obj.split(数据集X，参照列y) 将数据集按照参照列进行分层，本质上是利用了参照列的分布\n",
    "for test_index, train_index in split_obj.split(housing,housing[\"income_cat\"]):\n",
    "    # 这个循环会执行 n_splits 那么多次，这里只执行一次\n",
    "    strat_train_set = housing.loc[train_index] # loc作用于数据的label上，iloc作用于数据存储的indexs上\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "# strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set) # value_counts得到这一列的分布\n",
    "# 通过dataframe.columns来查看所有列\n",
    "strat_train_set.columns\n",
    "# 需要删除掉 income_cat 这个属性，因为这是人工加上去的\n",
    "for set_ in (strat_train_set,strat_test_set):\n",
    "    set_.drop(\"income_cat\",axis=1,inplace=True) # axis指明这是按列删除，前面的“income_cat”表示该列的标识\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836f5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                      # Prepare the Data for Machine Learning Algorithms (数据预处理)#\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439003e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先分离标签数据和参考数据\n",
    "housing = strat_train_set.drop(\"median_house_value\",axis=1) # drop 操作会拷贝，所以本身不会变化\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "housing_num = housing.drop(\"ocean_proximity\",axis=1) # 在参考数据中分离出类别数据和数值数据\n",
    "####### 先做一个自定义的 Transfromer 后面会用到 ###############\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "class CombineAttributesAddr(BaseEstimator,TransformerMixin): # 括号里面表示继承的基类\n",
    "    def __init__(self,add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room # add_bedrooms_per_room 是一个超参\n",
    "    def fit(self,X,y = None):\n",
    "        # 实现fit方法\n",
    "        return self;\n",
    "    def transform(self,X,y = None):\n",
    "        # 实现transform方法\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room] # np.c_用于按列连接两个矩阵\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "###############################################################\n",
    "#### 4. Transformation Pipelines\n",
    "   # Pipeline的思想就是将上面所有对数据的操作连接起来\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('add_addr',CombineAttributesAddr()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])# Pipeline的参数是一个list，每个元素是一个tuple,指明名字和使用的estimator，最后一个tuple得是一个transformer\n",
    "# 然后直接调用num_pipeline的fit_transform方法传入housing的数值数据就可以一步完成上面的空值填充，属性组合，标准化等操作了\n",
    "# 但是上面的pipeline只是对于数值数据进行操作，还希望对housing数据中类别数据进行transform,\n",
    "from sklearn.compose import ColumnTransformer\n",
    "num_attr = list(housing_num) # 获取housing_num的所有列名\n",
    "cate_attr = [\"ocean_proximity\"] # 类型一致\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\",num_pipeline,num_attr),\n",
    "    (\"cate\",OneHotEncoder(),cate_attr)\n",
    "]) # ColumnTransformer的参数是一个list，每一个元素是一个tuple，tuple里面分别是名称，transformer,该transformer要transform的列名们\n",
    "\n",
    "# 最后调用一下就OK\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f8e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                                 # Select and Train a Model #\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89f189c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### 1. Training and Evaluating on the Training Set\n",
    "###  首先看下线性回归的效果\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_gre = LinearRegression()\n",
    "lin_gre.fit(housing_prepared,housing_labels) # 线性回归模型的fit方法用于拟合，参数是样本和标签\n",
    "###  看看训练出来的模型的误差，使用均方根误差(实际上就是概率意义上的标准差)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_gre.predict(housing_prepared)\n",
    "housing_mse = mean_squared_error(housing_labels,housing_predictions)\n",
    "housing_rmse = np.sqrt(housing_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e754949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66787.70814784677"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b217141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 66787这样的误差相对于原始数据的120000到265000的尺度来说是0.1，有点大了\n",
    "### 这种就属于是underfitting, 我们可以换一个模型试试\n",
    "### 下面我们换一个更为强大的模型----决策树模型\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "deci_tree = DecisionTreeRegressor()\n",
    "deci_tree.fit(housing_prepared,housing_labels)\n",
    "housing_tree_predictions = deci_tree.predict(housing_prepared)\n",
    "housing_tree_mse = mean_squared_error(housing_labels,housing_tree_predictions)\n",
    "housing_tree_rmse = np.sqrt(housing_tree_mse)\n",
    "housing_tree_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fbe5d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [74879.43164092 81368.03511853 73001.08264174 73435.6217542\n",
      " 72032.69365025 71588.7109206  76887.83841054 78661.57960908\n",
      " 71823.13510265 79175.04984221]\n",
      "Mean: 75285.31786907338\n",
      "Standard deviation: 3333.6422431393994\n"
     ]
    }
   ],
   "source": [
    "### 看到这里误差为0 就明白极大可能是过拟合了，但我们还是不敢确定，所以要进行验证\n",
    "### 但是验证的话我们不能动测试集中的数据，所以我们把训练集中的数据一部分用来训练，一部分用来验证(validation)\n",
    "\n",
    "### 2. Better Evaluation Using Cross-Validation\n",
    "  ### 在交叉验证前，一般是先将训练集数据分为k个部分，这个k称为 折(fold) ，在交叉验证中, 一般需要进行k次训练和验证，\n",
    "  ### 每次从k个fold中选择一个fold作为验证集，其余的k-1个fold作为训练集，一次的训练和验证产生一个训练-验证分数(实际上就是一个误差)\n",
    "  ### 然后再选择另外一个不同的fold作为验证集，以此类推。最后得到一个长度为k的训练-验证分数向量\n",
    "  ### 交叉验证可以让我们看到所选择的模型的预测能力，还能够通过标准差来反映这种预测能力的准确性(它产生的误差范围)\n",
    "\n",
    "  ### 使用sklearn.model_selection的cross_val_score可以完成交叉验证功能\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 第一个参数是分类器，第二三个参数分别是：数据特征和标签，第四个参数是计算分数的方法，cv是折数\n",
    "scores = cross_val_score(deci_tree,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4232a608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [66830.21497764 73595.46286068 67888.5957665  69274.18198153\n",
      " 66475.99624101 66144.88036095 64702.02705501 65639.32837098\n",
      " 67105.20129799 67421.10791841]\n",
      "Mean: 67507.69968307031\n",
      "Standard deviation: 2350.3018828733475\n"
     ]
    }
   ],
   "source": [
    "### 可以看到 分数的均值甚至比线性回归模型还要拉胯，为了公平一些，我们还是看看线性回归模型在交叉验证下的表现\n",
    "linea_scores = cross_val_score(lin_gre,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "linea_rmse_scores = np.sqrt(-linea_scores)\n",
    "display_scores(linea_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b67429c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20035.380599451564"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 看吧看吧，看来线性回归模型还是要比决策树模型要好的\n",
    "### 但是线性回归模型underfitting了不是，那我们还是再看看其他模型\n",
    "\n",
    "### 接下来采用随机森林回归模型\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared,housing_labels)\n",
    "forest_prediction = forest_reg.predict(housing_prepared)\n",
    "forest_rmse = np.sqrt(mean_squared_error(housing_labels,forest_prediction))\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b91b3d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [52833.08322075 55233.75274663 52902.27036074 51296.78623101\n",
      " 50660.96400371 52579.21961832 53445.77661821 54557.04524026\n",
      " 51952.41219146 59640.10331805]\n",
      "Mean: 53510.14135491365\n",
      "Standard deviation: 2424.5664197054184\n"
     ]
    }
   ],
   "source": [
    "### 可以看到比线性回归模型好了很多了，那在验证集下(交叉验证)的表现如何呢？\n",
    "forest_scores = cross_val_score(forest_reg,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "611abebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 可以看到均值已经好了很多了，但是在验证集下的结果比训练集下的结果更拉胯(得多)，说明overfitting了\n",
    "\n",
    "### 现在整理一下思路，在线性模型上由于在训练集上的结果就很拉胯说明underfitting了\n",
    "### 换了一个决策树模型，结果训练集上表现完美，但是验证集上表现比线性模型更拉胯，说明overfitting了\n",
    "### 又换了一个随机森林模型，结果训练集上表现相对于线性模型要好一些了，而验证集上的该模型又比决策树模型好了一些，但是还是由于验证集比训练集烂太多而overfitting了\n",
    "\n",
    "### 我们可以再试试其他的模型，文中提出在选择模型阶段我们一般选出2-5个表现比较好的模型就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9caa748a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1901056836.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\siran\\AppData\\Local\\Temp\\ipykernel_11232\\1901056836.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    .\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 很多时候我们应该要保存我们选择出来的模型(训练好了的)\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(my_model, \"my_model.pkl\") ## 保存\n",
    ".\n",
    "# my_model_loaded = joblib.load(\"my_model.pkl\") ## 读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                                     # Fine-Tune Your Model #\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "136b4ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 4, 'n_estimators': 30}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 拥有了选择好的一些模型后，我们需要对这些模型进行微调(fine-tune)\n",
    "##### 1. 一种进行微调的方法就是调整超参数，这里用到的是网格搜索(没错，就是基于笛卡尔积的那种网格)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{\"n_estimators\":[3,10,30],\"max_features\":[2,4,6,8]},\n",
    "              {\"bootstrap\":[False],\"n_estimators\":[3,10],\"max_features\":[2,3,4]}]\n",
    "# 这里 param_grid 是要作为参数传入GridSearchCV中的\n",
    "# 列表里面的每一个字典都是一组需要测试的超参: \n",
    "###### 'n_estimators'，'max_features'，'bootstrap'是超参们，它们的意义后面会介绍，这里它们的两个列表会做一个笛卡尔积的操作，即：\n",
    "######  [3,2],[3,4],[3,6]，...它们作为超参组合被用于训练以及验证\n",
    "######  第二个字典还增加了bootstrap为false的超参进去进行对比， 默认是True的\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg,param_grid,cv=5,scoring=\"neg_mean_squared_error\",return_train_score=True)\n",
    "# 你应该注意到了 几乎所有的模式都是先按照某些超参把这个模型建立起来，然后再让这个模型去fit数据\n",
    "grid_search.fit(housing_prepared,housing_labels)\n",
    "# 通过训练后可以通过grid_search的best_params_属性来得到找到的最佳超参\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ab64ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=4, n_estimators=30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可通过grid_search的best_estimator_来看最佳的预测器的超参设置\n",
    "grid_search.best_estimator_\n",
    "# 更多用法可见 https://www.cnblogs.com/dalege/p/14175192.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 但是需要指出网格搜索对于庞大的超参数空间是十分耗时的\n",
    "###### 所以此时可以选用 RandomSearchCV，通过设置迭代步数来对超参数的组合数进行限制而达到减小耗时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ec4db1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0796786 , 0.06863641, 0.0380639 , 0.02671849, 0.0251539 ,\n",
       "       0.02718335, 0.02206064, 0.27211934, 0.07449396, 0.09475054,\n",
       "       0.09198812, 0.0200512 , 0.14499885, 0.00094227, 0.00575357,\n",
       "       0.00740688])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### 2. 另一种对模型进行微调的方法是将表现得很好的一些模型进行组合\n",
    "###### 进行如上操作首先需要分析表现得很好的模型\n",
    "# 查看表现得很好的模型的特征参数\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "830e0cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.27211933755738393, 'median_income'),\n",
       " (0.1449988541688845, 'INLAND'),\n",
       " (0.09475053885835279, 'pop_per_hhold'),\n",
       " (0.0919881151196966, 'bedrooms_per_room'),\n",
       " (0.07967859615615107, 'longitude'),\n",
       " (0.07449395650866183, 'rooms_per_hhold'),\n",
       " (0.06863640546779907, 'latitude'),\n",
       " (0.038063896515509926, 'housing_median_age'),\n",
       " (0.02718334516160155, 'population'),\n",
       " (0.026718486033989207, 'total_rooms'),\n",
       " (0.025153902112110205, 'total_bedrooms'),\n",
       " (0.022060644504249712, 'households'),\n",
       " (0.02005120137117593, '<1H OCEAN'),\n",
       " (0.007406884646096861, 'NEAR OCEAN'),\n",
       " (0.005753565735625612, 'NEAR BAY'),\n",
       " (0.0009422700827112013, 'ISLAND')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cate\"] # 现在当初起得名字就有作用了\n",
    "cat_one_hot_attr = list(cat_encoder.categories_[0]) # categories_中包含了所有类特征的所有值，由于只有一个类特征，所以只需要取第一个就可以了\n",
    "attrs = num_attr+extra_attribs+cat_one_hot_attr\n",
    "sorted(zip(feature_importances,attrs),reverse=True) # zip(a,b)将a,b这两个可迭代对象中的元素一个个打包成一个元组，返回一个元组的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51592b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                                # Evaluate Your System on the Test Set #\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "273f03dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54503.17006937851"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 经过上面的训练，我们找出了当前环境下表现最好的模型，然后我们就需要对这个模型在测试集上进行验证\n",
    "### 注意我们是在验证集上进行交叉验证选择了比较好的一些模型，然后对这些模型又进行了交叉验证来进行超参选择等的fine-tune后来到了这里\n",
    "# 首先还是需要将测试集的特征和标签分开，然后把特征transform一下\n",
    "X_test = strat_test_set.drop(\"median_house_value\",axis=1)\n",
    "Y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_test_transformed = full_pipeline.transform(X_test) # 注意这里不能使用 fit_transform 因为测试集上你绝对不能fit\n",
    "# 召唤出训练好的，调整好超参的模型\n",
    "final_model = grid_search.best_estimator_\n",
    "final_predictions = final_model.predict(X_test_transformed)\n",
    "final_rmse = np.sqrt(mean_squared_error(Y_test,final_predictions))\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7326358c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53385.1304235 , 55598.73150432])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 如何判断这个预测的准确性呢？\n",
    "### 我们考虑预测均方误差的范围\n",
    "### 通过置信区间的来看看\n",
    "from scipy import stats\n",
    "confidence = 0.95 # 设置置信度\n",
    "squared_errors = (final_predictions-Y_test)**2\n",
    "np.sqrt(stats.t.interval(confidence,len(squared_errors)-1, loc=squared_errors.mean(),scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51304f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 也就是说预测的均方误差有95%的可能性落在长度为其十分之一区间中，误差取得的区间比较小，说明预测还是比较集中的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879903a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                            # Launch, Monitor, and Maintain Your System #\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb83073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
