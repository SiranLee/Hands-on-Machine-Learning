{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                                     # Reading the Data #\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57e05263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd # 导入pandas库，用pd作别名\n",
    "HOUSING_PATH = os.path.join(\"datasets\",\"housing\") # 指定下载数据到的目录 datasets/housing/\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path,\"housing.csv\")\n",
    "    return pd.read_csv(csv_path) # 返回一个包含所有数据的 pandas 数据框架对象\n",
    "housing = load_housing_data() # 返回一个包含所有数据的 pandas 数据框架对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                                     # Create a Test set #\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7eadbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分层抽样 根据收入中位数来进行分层抽样\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 将 median_income 的数据按照bins来划分，并给每个区间中的数据打上label\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],bins=[0.,1.5,3,4.5,6.,np.inf],labels=[1,2,3,4,5])\n",
    "# 现在层次已经分出来了，我们要做的就是在每一层选择一些样本出来\n",
    "# sklearn 提供了 StratifiedShuffleSplit 函数来进行分层抽样\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# n_splits 表示只分成一对 test/train 集，返回的还是一个 StratifiedShuffleSplit 对象\n",
    "split_obj = StratifiedShuffleSplit(n_splits=1,test_size=0.2, random_state=42)\n",
    "# split_obj.split(数据集X，参照列y) 将数据集按照参照列进行分层，本质上是利用了参照列的分布\n",
    "for test_index, train_index in split_obj.split(housing,housing[\"income_cat\"]):\n",
    "    # 这个循环会执行 n_splits 那么多次，这里只执行一次\n",
    "    strat_train_set = housing.loc[train_index] # loc作用于数据的label上，iloc作用于数据存储的indexs上\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "# strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set) # value_counts得到这一列的分布\n",
    "# 通过dataframe.columns来查看所有列\n",
    "strat_train_set.columns\n",
    "# 需要删除掉 income_cat 这个属性，因为这是人工加上去的\n",
    "for set_ in (strat_train_set,strat_test_set):\n",
    "    set_.drop(\"income_cat\",axis=1,inplace=True) # axis指明这是按列删除，前面的“income_cat”表示该列的标识\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                      # Prepare the Data for Machine Learning Algorithms (数据预处理)#\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "439003e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先分离标签数据和参考数据\n",
    "housing = strat_train_set.drop(\"median_house_value\",axis=1) # drop 操作会拷贝，所以本身不会变化\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "housing_num = housing.drop(\"ocean_proximity\",axis=1) # 在参考数据中分离出类别数据和数值数据\n",
    "####### 先做一个自定义的 Transfromer 后面会用到 ###############\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "class CombineAttributesAddr(BaseEstimator,TransformerMixin): # 括号里面表示继承的基类\n",
    "    def __init__(self,add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room # add_bedrooms_per_room 是一个超参\n",
    "    def fit(self,X,y = None):\n",
    "        # 实现fit方法\n",
    "        return self;\n",
    "    def transform(self,X,y = None):\n",
    "        # 实现transform方法\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room] # np.c_用于按列连接两个矩阵\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "###############################################################\n",
    "#### 4. Transformation Pipelines\n",
    "   # Pipeline的思想就是将上面所有对数据的操作连接起来\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "    ('add_addr',CombineAttributesAddr()),\n",
    "    ('std_scaler',StandardScaler())\n",
    "])# Pipeline的参数是一个list，每个元素是一个tuple,指明名字和使用的estimator，最后一个tuple得是一个transformer\n",
    "# 然后直接调用num_pipeline的fit_transform方法传入housing的数值数据就可以一步完成上面的空值填充，属性组合，标准化等操作了\n",
    "# 但是上面的pipeline只是对于数值数据进行操作，还希望对housing数据中类别数据进行transform,\n",
    "from sklearn.compose import ColumnTransformer\n",
    "num_attr = list(housing_num) # 获取housing_num的所有列名\n",
    "cate_attr = [\"ocean_proximity\"] # 类型一致\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\",num_pipeline,num_attr),\n",
    "    (\"cate\",OneHotEncoder(),cate_attr)\n",
    "]) # ColumnTransformer的参数是一个list，每一个元素是一个tuple，tuple里面分别是名称，transformer,该transformer要transform的列名们\n",
    "\n",
    "# 最后调用一下就OK\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################################\n",
    "\n",
    "                                                 # Select and Train a Model #\n",
    "    \n",
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d89f189c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### 1. Training and Evaluating on the Training Set\n",
    "###  首先看下线性回归的效果\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_gre = LinearRegression()\n",
    "lin_gre.fit(housing_prepared,housing_labels) # 线性回归模型的fit方法用于拟合，参数是样本和标签\n",
    "###  看看训练出来的模型的误差，使用均方根误差(实际上就是概率意义上的标准差)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_gre.predict(housing_prepared)\n",
    "housing_mse = mean_squared_error(housing_labels,housing_predictions)\n",
    "housing_rmse = np.sqrt(housing_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e754949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66787.70814784677"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b217141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 66787这样的误差相对于原始数据的120000到265000的尺度来说是0.1，有点大了\n",
    "### 这种就属于是underfitting, 我们可以换一个模型试试\n",
    "### 下面我们换一个更为强大的模型----决策树模型\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "deci_tree = DecisionTreeRegressor()\n",
    "deci_tree.fit(housing_prepared,housing_labels)\n",
    "housing_tree_predictions = deci_tree.predict(housing_prepared)\n",
    "housing_tree_mse = mean_squared_error(housing_labels,housing_tree_predictions)\n",
    "housing_tree_rmse = np.sqrt(housing_tree_mse)\n",
    "housing_tree_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 看到这里误差为0 就明白极大可能是过拟合了，但我们还是不敢确定，所以要进行验证\n",
    "### 但是验证的话我们不能动测试集中的数据，所以我们把训练集中的数据一部分用来训练，一部分用来验证(validation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
